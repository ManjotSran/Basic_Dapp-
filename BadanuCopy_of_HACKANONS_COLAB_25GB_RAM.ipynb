{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManjotSran/Basic_Dapp-/blob/main/BadanuCopy_of_HACKANONS_COLAB_25GB_RAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from unicodedata import normalize\n",
        "from numpy import array"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open('pecorpus.txt', mode='rt', encoding='utf-8')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n"
      ],
      "metadata": {
        "id": "SGkEhmWADNuN",
        "outputId": "62fcf022-da16-46ed-b49e-d136aaca4e8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m235.5/235.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install indic-transliteration\n"
      ],
      "metadata": {
        "id": "kZrELmt_EM-H",
        "outputId": "1c45d00a-153e-4c42-f7a9-e1cc5fd67034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting indic-transliteration\n",
            "  Downloading indic_transliteration-2.3.44-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backports.functools-lru-cache (from indic-transliteration)\n",
            "  Downloading backports.functools_lru_cache-1.6.5-py2.py3-none-any.whl (6.0 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (2022.10.31)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.10.2)\n",
            "Collecting roman (from indic-transliteration)\n",
            "  Downloading roman-4.1-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (8.1.3)\n",
            "Installing collected packages: roman, backports.functools-lru-cache, indic-transliteration\n",
            "Successfully installed backports.functools-lru-cache-1.6.5 indic-transliteration-2.3.44 roman-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "import pickle\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# split a loaded document into sentence pairs\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for i, line in enumerate(pair):\n",
        "            # split on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if unidecode(word).isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return np.array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(sentences, f)\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'pecorpus.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into Punjabi-English pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "cleaned_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(cleaned_pairs, 'punjabi-english.pkl')\n",
        "\n",
        "# spot check\n",
        "for i in range(100):\n",
        "    print('[%s] => [%s]' % (cleaned_pairs[i, 0], cleaned_pairs[i, 1]))\n"
      ],
      "metadata": {
        "id": "WgGgN1pt_o0Z",
        "outputId": "40c8e9df-d4c5-4e48-99ad-319f11795dca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: punjabi-english.pkl\n",
            "[ਹੈਲੋ] => [hi]\n",
            "[ਹੈਲੋ] => [hi]\n",
            "[ਰਨ] => [run]\n",
            "[ਵਾਹ] => [wow]\n",
            "[ਵਾਹ] => [wow]\n",
            "[ਅੱਗ] => [fire]\n",
            "[ਮਦਦ ਕਰੋ] => [help]\n",
            "[ਮਦਦ ਕਰੋ] => [help]\n",
            "[ਰੂਕੋ] => [stop]\n",
            "[ਉਡੀਕ ਕਰੋ] => [wait]\n",
            "[ਸਤ ਸ੍ਰੀ ਅਕਾਲ] => [hello]\n",
            "[ਮੈਂ ਕਰਦਾ ਹਾਂ] => [i try]\n",
            "[ਮੈਂ ਜਿੱਤਿਆ] => [i won]\n",
            "[ਮੈਂ ਜਿੱਤਿਆ] => [i won]\n",
            "[ਮੁਸਕਰਾਓ] => [smile]\n",
            "[ਚੀਰਸ] => [cheers]\n",
            "[] => [freeze]\n",
            "[] => [freeze]\n",
            "[ਮਿਲ ਗਿਆ] => [got it]\n",
            "[ਮਿਲ ਗਿਆ] => [got it]\n",
            "[ਓੁਹ ਭਁਜਿਆ] => [he ran]\n",
            "[ਓੁਹ ਭਁਜਿਆ] => [he ran]\n",
            "[ਅੰਦਰ ਆ ਜਾਓ] => [hop in]\n",
            "[ਮੈਨੂੰ ਜੱਫੀ ਪਾਓ] => [hug me]\n",
            "[ਮੈਨੂੰ ਜੱਫੀ ਪਾਓ] => [hug me]\n",
            "[ਮੈਨੂੰ ਜੱਫੀ ਪਾਓ] => [hug me]\n",
            "[ਮੈਂ ਡਿੱਗ ਪਿਆ] => [i fell]\n",
            "[ਮੈਂ ਡਿੱਗ ਪਿਆ] => [i fell]\n",
            "[ਮੈਂ ਡਿੱਗ ਪਿਆ] => [i fell]\n",
            "[ਮੈਂ ਡਿੱਗ ਪਿਆ] => [i fell]\n",
            "[ਮੈਂ ਡਿੱਗ ਪਿਆ] => [i fell]\n",
            "[ਮੈਨੂੰ ਪਤਾ ਹੈ] => [i know]\n",
            "[ਮੈਂ ਝੂਠ ਬੋਲਿਆ] => [i lied]\n",
            "[ਮੈਂ ਹਾਰ ਗਿਆ] => [i lost]\n",
            "[ਮੈਂ ਸਾਲ ਦਾ ਹਾਂ] => [im]\n",
            "[ਮੈਂ ਸਾਲ ਦਾ ਹਾਂ] => [im]\n",
            "[ਮੈ ਠੀਕ ਹਾਂ] => [im ok]\n",
            "[ਮੈ ਠੀਕ ਹਾਂ] => [im ok]\n",
            "[ਹੋ ਨਹੀਂ ਸਕਦਾ] => [no way]\n",
            "[ਹੋ ਨਹੀਂ ਸਕਦਾ] => [no way]\n",
            "[ਹੋ ਨਹੀਂ ਸਕਦਾ] => [no way]\n",
            "[ਹੋ ਨਹੀਂ ਸਕਦਾ] => [no way]\n",
            "[ਸੱਚਮੁੱਚ] => [really]\n",
            "[ਸੱਚਮੁੱਚ] => [really]\n",
            "[ਸੱਚਮੁੱਚ] => [really]\n",
            "[ਧੰਨਵਾਦ] => [thanks]\n",
            "[ਇਸਨੂੰ] => [try it]\n",
            "[ਮੈਂ ਹੀ ਕਿਓਂ] => [why me]\n",
            "[ਟੌਮ ਨੂੰ ਪੁੱਛੋ] => [ask tom]\n",
            "[ਟੌਮ ਨੂੰ ਪੁੱਛੋ] => [ask tom]\n",
            "[ਟੌਮ ਨੂੰ ਪੁੱਛੋ] => [ask tom]\n",
            "[ਠੰਡ ਰੱਖ] => [be cool]\n",
            "[ਨਿਰਪੱਖ ਰਹੋ] => [be fair]\n",
            "[ਨਿਰਪੱਖ ਰਹੋ] => [be fair]\n",
            "[ਚੰਗੇ ਬਣੋ] => [be nice]\n",
            "[ਚੰਗੇ ਬਣੋ] => [be nice]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਹਰਾ ਦੋ ਇਸਨੂੰ] => [beat it]\n",
            "[ਮੈਨੂੰ ਕਾਲ ਕਰੋ] => [call me]\n",
            "[ਅੰਦਰ ਆ ਜਾਓ] => [come in]\n",
            "[ਅੰਦਰ ਆ ਜਾਓ] => [come in]\n",
            "[ਆ ਜਾਓ] => [come on]\n",
            "[ਆ ਜਾਓ] => [come on]\n",
            "[ਆ ਜਾਓ] => [come on]\n",
            "[ਆ ਜਾਓ] => [come on]\n",
            "[ਹੋ ਜਾਓ] => [get out]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n",
            "[ਚਲੇ ਜਾਓ] => [go away]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('punjabi-english.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 30000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "\n",
        "# split into train/test\n",
        "train, test = dataset[:27000], dataset[27000:]\n",
        "\n",
        "# save\n",
        "save_clean_data(dataset, 'punjabi-english-both.pkl')\n",
        "save_clean_data(train, 'punjabi-english-train.pkl')\n",
        "save_clean_data(test, 'punjabi-english-test.pkl')\n"
      ],
      "metadata": {
        "id": "QRrm_2PjF8Gp",
        "outputId": "ea21bd54-4f95-4822-d610-09deb4250922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: punjabi-english-both.pkl\n",
            "Saved: punjabi-english-train.pkl\n",
            "Saved: punjabi-english-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "WkUPjF7bI5L3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=(input_shape[0][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=(input_shape[1][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=(input_shape[0][2], 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "JlJYGVQ1OE3k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed, Concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n"
      ],
      "metadata": {
        "id": "D727ZTM4b6f1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(src_timesteps,))\n",
        "    enc_emb =  Embedding(src_vocab, n_units, mask_zero = True)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_emb_layer = Embedding(tar_vocab, n_units, mask_zero = True)\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "    # Attention Layer\n",
        "    attn_layer = AttentionLayer()\n",
        "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "    # Concat attention input and decoder LSTM output\n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "    # Dense layer\n",
        "    dense = Dense(tar_vocab, activation='softmax')\n",
        "    dense_time = TimeDistributed(dense)\n",
        "    decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "    # Full model\n",
        "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "    #model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('punjabi-english-both.pkl')\n",
        "train = load_clean_sentences('punjabi-english-train.pkl')\n",
        "test = load_clean_sentences('punjabi-english-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 1])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % eng_length)\n",
        "\n",
        "# prepare punjabi tokenizer\n",
        "pun_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "pun_vocab_size = len(pun_tokenizer.word_index) + 1\n",
        "pun_length = max_length(dataset[:, 0])\n",
        "print('Punjabi Vocabulary Size: %d' % pun_vocab_size)\n",
        "print('Punjabi Max Length: %d' % pun_length)\n",
        "\n",
        "# prepare training data\n",
        "#trainX = encode_sequences(pun_tokenizer, pun_length, train[:, 0])\n",
        "#trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 1])\n",
        "# reshape the targets to have shape (batch_size, sequence_length, 1)\n",
        "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], 1)\n",
        "testY = testY.reshape(testY.shape[0], testY.shape[1], 1)\n",
        "\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(pun_tokenizer, pun_length, test[:, 0])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 1])\n",
        "\n",
        "# define model\n",
        "model = define_model(pun_vocab_size, eng_vocab_size, pun_length, eng_length, 256)\n",
        "\n",
        "# fit model\n",
        "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([trainX, trainY[:,:-1]], trainY[:,1:], epochs=30, batch_size=64, validation_data=([testX, testY[:,:-1]], testY[:,1:]), callbacks=[checkpoint], verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGNsNdsbrUp",
        "outputId": "be9c0c29-3c42-436d-9e35-bde6df352e96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 4937\n",
            "English Max Length: 7\n",
            "Punjabi Vocabulary Size: 4766\n",
            "Punjabi Max Length: 11\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 11, 256)      1220096     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    1263872     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 11, 256),    525312      ['embedding_2[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[0][0]',            \n",
            "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " attention_layer_1 (AttentionLa  ((None, None, 256),  131328     ['lstm_2[0][0]',                 \n",
            " yer)                            (None, None, 11))                'lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 512)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention_layer_1[0][0]']      \n",
            "                                                                                                  \n",
            " time_distributed_1 (TimeDistri  (None, None, 4937)  2532681     ['concat_layer[0][0]']           \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,198,601\n",
            "Trainable params: 6,198,601\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.59848, saving model to model.h5\n",
            "422/422 - 116s - loss: 5.0638 - val_loss: 4.5985 - 116s/epoch - 276ms/step\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 4.59848 to 4.06961, saving model to model.h5\n",
            "422/422 - 109s - loss: 4.2989 - val_loss: 4.0696 - 109s/epoch - 258ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 4.06961 to 3.62942, saving model to model.h5\n",
            "422/422 - 107s - loss: 3.7995 - val_loss: 3.6294 - 107s/epoch - 254ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.62942 to 3.34189, saving model to model.h5\n",
            "422/422 - 107s - loss: 3.4167 - val_loss: 3.3419 - 107s/epoch - 254ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 3.34189 to 3.11376, saving model to model.h5\n",
            "422/422 - 107s - loss: 3.1092 - val_loss: 3.1138 - 107s/epoch - 253ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 3.11376 to 2.87714, saving model to model.h5\n",
            "422/422 - 107s - loss: 2.8443 - val_loss: 2.8771 - 107s/epoch - 255ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 2.87714 to 2.68438, saving model to model.h5\n",
            "422/422 - 107s - loss: 2.6051 - val_loss: 2.6844 - 107s/epoch - 254ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 2.68438 to 2.50625, saving model to model.h5\n",
            "422/422 - 108s - loss: 2.3895 - val_loss: 2.5062 - 108s/epoch - 256ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 2.50625 to 2.39463, saving model to model.h5\n",
            "422/422 - 108s - loss: 2.1946 - val_loss: 2.3946 - 108s/epoch - 255ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.39463 to 2.24671, saving model to model.h5\n",
            "422/422 - 108s - loss: 2.0170 - val_loss: 2.2467 - 108s/epoch - 256ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 2.24671 to 2.14076, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.8548 - val_loss: 2.1408 - 108s/epoch - 257ms/step\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 2.14076 to 2.03889, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.7039 - val_loss: 2.0389 - 108s/epoch - 256ms/step\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 2.03889 to 1.95409, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.5679 - val_loss: 1.9541 - 108s/epoch - 255ms/step\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 1.95409 to 1.87673, saving model to model.h5\n",
            "422/422 - 107s - loss: 1.4445 - val_loss: 1.8767 - 107s/epoch - 254ms/step\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 1.87673 to 1.80093, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.3308 - val_loss: 1.8009 - 108s/epoch - 256ms/step\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 1.80093 to 1.76050, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.2272 - val_loss: 1.7605 - 108s/epoch - 256ms/step\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 1.76050 to 1.68445, saving model to model.h5\n",
            "422/422 - 109s - loss: 1.1354 - val_loss: 1.6844 - 109s/epoch - 257ms/step\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 1.68445 to 1.63910, saving model to model.h5\n",
            "422/422 - 108s - loss: 1.0499 - val_loss: 1.6391 - 108s/epoch - 255ms/step\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 1.63910 to 1.59071, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.9706 - val_loss: 1.5907 - 107s/epoch - 253ms/step\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 1.59071 to 1.55397, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.9002 - val_loss: 1.5540 - 107s/epoch - 254ms/step\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss improved from 1.55397 to 1.52479, saving model to model.h5\n",
            "422/422 - 108s - loss: 0.8323 - val_loss: 1.5248 - 108s/epoch - 255ms/step\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 1.52479 to 1.51106, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.7709 - val_loss: 1.5111 - 107s/epoch - 253ms/step\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 1.51106 to 1.47037, saving model to model.h5\n",
            "422/422 - 108s - loss: 0.7158 - val_loss: 1.4704 - 108s/epoch - 255ms/step\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss improved from 1.47037 to 1.45988, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.6652 - val_loss: 1.4599 - 107s/epoch - 254ms/step\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss improved from 1.45988 to 1.44172, saving model to model.h5\n",
            "422/422 - 108s - loss: 0.6171 - val_loss: 1.4417 - 108s/epoch - 255ms/step\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss improved from 1.44172 to 1.41872, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.5739 - val_loss: 1.4187 - 107s/epoch - 254ms/step\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss improved from 1.41872 to 1.40341, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.5323 - val_loss: 1.4034 - 107s/epoch - 254ms/step\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss improved from 1.40341 to 1.38569, saving model to model.h5\n",
            "422/422 - 106s - loss: 0.4948 - val_loss: 1.3857 - 106s/epoch - 251ms/step\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.38569\n",
            "422/422 - 107s - loss: 0.4603 - val_loss: 1.3887 - 107s/epoch - 254ms/step\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss improved from 1.38569 to 1.36427, saving model to model.h5\n",
            "422/422 - 107s - loss: 0.4281 - val_loss: 1.3643 - 107s/epoch - 254ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb15b394310>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pandas as pd\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=(input_shape[0][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=(input_shape[1][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=(input_shape[0][2], 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return load(f)\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence(model, eng_tokenizer, source, source2):\n",
        "    # source2 is the second input your model expects\n",
        "    prediction = model.predict([source, source2], verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, eng_tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "def evaluate_model(model, sources, sources2, eng_tokenizer, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        source2 = sources2[i].reshape((1, sources2[i].shape[0])) # reshape the second source\n",
        "        # translate encoded source text\n",
        "        translation = predict_sequence(model, eng_tokenizer, source, source2) # pass source2 to the predict_sequence function\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('punjabi-english-both.pkl')\n",
        "train = load_clean_sentences('punjabi-english-train.pkl')\n",
        "test = load_clean_sentences('punjabi-english-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 1])\n",
        "\n",
        "# prepare punjabi tokenizer\n",
        "pun_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "pun_vocab_size = len(pun_tokenizer.word_index) + 1\n",
        "pun_length = max_length(dataset[:, 0])\n",
        "\n",
        "# prepare data\n",
        "trainX = encode_sequences(pun_tokenizer, pun_length, train[:, 0])\n",
        "testX = encode_sequences(pun_tokenizer, pun_length, test[:, 0])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "\n",
        "# test on some training sequences\n",
        "#print('train')\n",
        "#evaluate_model(model, trainX, eng_tokenizer, train)\n",
        "\n",
        "# test on some test sequences\n",
        "#print('test')\n",
        "#evaluate_model(model, testX, eng_tokenizer, test)\n",
        "trainX2 = encode_sequences(eng_tokenizer, eng_length, train[:, 1])\n",
        "testX2 = encode_sequences(eng_tokenizer, eng_length, test[:, 1])\n",
        "\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, trainX, trainX2, eng_tokenizer, train)\n",
        "\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, testX, testX2, eng_tokenizer, test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdt_ZU-UucYx",
        "outputId": "bae522b3-4d98-40b2-be4b-6cb43dc29bef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[give me two minutes], target=[ਮੈਨੂੰ ਦੋ ਮਿੰਟ ਦਿਓ], predicted=[me two]\n",
            "src=[im really hungry], target=[ਮੈਨੂੰ ਸੱਚਮੁੱਚ ਭੁੱਖ ਲੱਗੀ ਹੈ], predicted=[really hungry]\n",
            "src=[she came], target=[ਉਹ ਆਈ], predicted=[came]\n",
            "src=[youre inconsiderate], target=[ਤੁਸੀਂ ਬੇਸਮਝ ਹੋ], predicted=[negligent]\n",
            "src=[we agree], target=[ਅਸੀਂ ਸਹਿਮਤ ਹਾਂ], predicted=[agree]\n",
            "src=[he acts quickly], target=[ਉਹ ਨਾਲ ਕੰਮ ਕਰਦਾ ਹੈ], predicted=[works quickly]\n",
            "src=[is it for here], target=[ਕੀ ਇਹ ਇੱਥੇ ਲਈ ਹੈ], predicted=[it for here]\n",
            "src=[i dont blame you], target=[ਮੈਂ ਤੁਹਾਨੂੰ ਨਹੀਂ ਦਿੰਦਾ], predicted=[dont blame you]\n",
            "src=[i hate tom], target=[ਮੈਂ ਟੌਮ ਨੂੰ ਕਰਦਾ ਹਾਂ], predicted=[hate tom]\n",
            "src=[we made a good buy], target=[ਅਸੀਂ ਚੰਗੀ ਖਰੀਦਦਾਰੀ ਕੀਤੀ], predicted=[got a good team]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.000175\n",
            "BLEU-2: 0.000000\n",
            "BLEU-3: 0.000000\n",
            "BLEU-4: 0.000000\n",
            "test\n",
            "src=[i can speak english], target=[ਮੈਂ ਬੋਲ ਸਕਦਾਸਕਦੀ ਹਾਂ], predicted=[can speak english]\n",
            "src=[dont touch me], target=[ਮੈਨੂੰ ਨਾ ਛੂਹੋ], predicted=[touch me]\n",
            "src=[tom seemed sad], target=[ਟੌਮ ਉਦਾਸ ਲੱਗ ਰਿਹਾ ਸੀ], predicted=[looked sad]\n",
            "src=[who is the boss here], target=[ਇੱਥੇ ਬੌਸ ਕੌਣ ਹੈ], predicted=[is here here here]\n",
            "src=[she made him cry], target=[ਉਸਨੇ ਉਸਨੂੰ ਰੋਇਆ], predicted=[made him cry]\n",
            "src=[please write it down], target=[ਕਿਰਪਾ ਕਰਕੇ ਇਸਨੂੰ ਲਿਖੋ], predicted=[make it]\n",
            "src=[i think he likes you], target=[ਮੈਨੂੰ ਲੱਗਦਾ ਹੈ ਕਿ ਉਹ ਤੁਹਾਨੂੰ ਪਸੰਦ ਕਰਦਾ ਹੈ], predicted=[think he likes hes]\n",
            "src=[he may well be right], target=[ਉਹ ਠੀਕ ਹੋ ਸਕਦਾ ਹੈ], predicted=[may well be right]\n",
            "src=[does tom have a fever], target=[ਕੀ ਟੌਮ ਨੂੰ ਬੁਖਾਰ ਹੈ], predicted=[tom have a fever]\n",
            "src=[can i go to bed now], target=[ਕੀ ਮੈਂ ਹੁਣ ਸੌਣ ਜਾ ਸਕਦਾ ਹਾਂ], predicted=[i go now bed now]\n",
            "BLEU-1: 0.000000\n",
            "BLEU-2: 0.000000\n",
            "BLEU-3: 0.000000\n",
            "BLEU-4: 0.000000\n"
          ]
        }
      ]
    }
  ]
}